{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2a2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script runs daily at 6pm\n",
    "# Scheduled using Azure Logic Apps\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pyodbc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53978502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MarketWatch.com shows instruments for Sri Lanka in the following URLs as two pages\n",
    "# https://www.marketwatch.com/tools/markets/stocks/country/sri-lanka\n",
    "# https://www.marketwatch.com/tools/markets/stocks/country/sri-lanka/2\n",
    "\n",
    "# Latter part of the instrument urls are added to a list (ex:['/investing/Stock/ABAN.N0000?countryCode=LK',\n",
    "#                                            '/investing/Stock/AFSL.N0000?countryCode=LK', etc...]\n",
    "\n",
    "def get_partial_instrument_links_from_page(url, maxlinks):\n",
    "    list_of_instrument_urls_in_page = []\n",
    "    items = requests.get(url)\n",
    "    soup_it = BeautifulSoup(items.content,\n",
    "                            'html5lib')\n",
    "\n",
    "    while len(list_of_instrument_urls_in_page) < maxlinks:\n",
    "        for links_page1 in soup_it.find_all('a'):\n",
    "            link = links_page1.get('href')\n",
    "\n",
    "            if link:\n",
    "                if '/investing/Stock/' in link:\n",
    "                    list_of_instrument_urls_in_page.append(link)\n",
    "    return list_of_instrument_urls_in_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92874823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_instrument_data_to_dic(dic, name_of_mkt_data, value_of_mkt_data):\n",
    "    if name_of_mkt_data not in dic:\n",
    "        dic[name_of_mkt_data] = []\n",
    "    dic[name_of_mkt_data].append(value_of_mkt_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f840779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs from page 1: 150\n",
      "URLs from page 2: 130\n"
     ]
    }
   ],
   "source": [
    "url_page1 = 'https://www.marketwatch.com/tools/markets/stocks/country/sri-lanka'\n",
    "url_page2 = 'https://www.marketwatch.com/tools/markets/stocks/country/sri-lanka/2'\n",
    "\n",
    "list_of_instruments_page1 = get_partial_instrument_links_from_page(url_page1, 150)\n",
    "list_of_instruments_page2 = get_partial_instrument_links_from_page(url_page2, 124)\n",
    "\n",
    "print(\"URLs from page 1: \" + str(len(list_of_instruments_page1)))\n",
    "print(\"URLs from page 2: \" + str(len(list_of_instruments_page2)))\n",
    "\n",
    "List_of_instruments = list_of_instruments_page1 + list_of_instruments_page2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be43dfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "# # # # # # # # # # # # #\n",
    "\n",
    "List_of_dic = []\n",
    "\n",
    "instrument_data = {}\n",
    "\n",
    "# The following for loop will:\n",
    "#           1. Create the instrument urls\n",
    "#           2. Scrape the webpage to get the needed information from the urls\n",
    "#           3. Add them to instrument_data dictionary (one dictionary to each instrument)\n",
    "#           4. Append the dictionaries to a list\n",
    "\n",
    "for instrument in List_of_instruments:\n",
    "    url = 'https://www.marketwatch.com' + instrument\n",
    "\n",
    "    r = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.content,\n",
    "                         'html5lib')\n",
    "    soup.prettify()\n",
    "\n",
    "    data_for_dic = soup.findAll(\"li\", {\"class\": \"kv__item\"})\n",
    "\n",
    "    for row in data_for_dic:\n",
    "        tag = row.find_all(\"small\", {\"class\": \"label\"})\n",
    "        value = row.find_all(\"span\", {\"class\": \"primary\"})\n",
    "\n",
    "        add_instrument_data_to_dic(instrument_data, tag[0].string, value[0].string)\n",
    "\n",
    "    instrument_data['Instrument'] = instrument[17:]\n",
    "\n",
    "    List_of_dic.append(instrument_data)\n",
    "\n",
    "    print(len(List_of_dic))\n",
    "\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a07ae917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a csv file using the data\n",
    "\n",
    "csv_columns = ['Open', 'Day Range', '52 Week Range', 'Market Cap',\n",
    "               'Shares Outstanding', 'Public Float', 'Beta', 'Rev. per Employee', 'P/E Ratio', 'EPS',\n",
    "               'Yield', 'Dividend', 'Ex-Dividend Date', 'Short Interest', '% of Float Shorted',\n",
    "               'Average Volume', 'Instrument']\n",
    "\n",
    "csv_file = \"data/Market_key_data.csv\"\n",
    "\n",
    "try:\n",
    "    with open(csv_file, 'w', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in List_of_dic:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c5aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data into Market_key_data_sanitized.csv after removing unnecessary characters and words in\n",
    "#                                                                                        Market_key_data.csv\n",
    "\n",
    "with open(\"data/Market_key_data.csv\", \"r\") as infile, open(\"Market_key_data_sanitized.csv\", \"w\") as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    conversion = set(\"]['\")\n",
    "    for row in reader:\n",
    "        newrow = [\"\".join(\"\" if c in conversion else c for c in entry) for entry in row]\n",
    "        for index, item in enumerate(newrow):\n",
    "            newrow[index] = newrow[index].replace(\"රු.\", \"\")\n",
    "            newrow[index] = newrow[index].replace(\"?countryCode=LK\", \"\")\n",
    "            newrow[index] = newrow[index].replace(\"N/A\", \"0\")\n",
    "            newrow[index] = newrow[index].replace(',', '')\n",
    "        writer.writerow(newrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c25125c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open                  object\n",
      "Day_Range             object\n",
      "my_52_Week_Range      object\n",
      "Market_Cap            object\n",
      "Shares_Outstanding    object\n",
      "Public_Float          object\n",
      "Beta                  object\n",
      "Rev_per_Employee      object\n",
      "P_E_Ratio             object\n",
      "EPS                   object\n",
      "Yield                 object\n",
      "Dividend              object\n",
      "Ex_Dividend_Date      object\n",
      "Short_Interest        object\n",
      "of_Float_Shorted      object\n",
      "Average_Volume        object\n",
      "Instrument            object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_19612/2767464830.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns = df.columns.str.replace('[/,-]', ' ')\n"
     ]
    }
   ],
   "source": [
    "# Feed the data into a pandas dataframe\n",
    "# Edit the column names to match the column names of the database\n",
    "# change the data types to match the database accordingly\n",
    "\n",
    "df = pd.read_csv(\"/Users/Dell/python files/Market_key_data_sanitized.csv\")\n",
    "df.columns = df.columns.str.replace('%', '')\n",
    "df.columns = df.columns.str.replace('[/,-]', ' ')\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "df.rename(columns={'52_Week_Range': 'my_52_Week_Range', '_of_Float_Shorted': 'of_Float_Shorted',\n",
    "                   'Rev._per_Employee': 'Rev_per_Employee'}, inplace=True)\n",
    "\n",
    "df[['Open', 'Beta', 'P_E_Ratio', 'EPS', 'Dividend', 'Short_Interest', 'of_Float_Shorted']] = df[\n",
    "    ['Open', 'Beta', 'P_E_Ratio', 'EPS', 'Dividend', 'Short_Interest', 'of_Float_Shorted']].astype(str)\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e1144b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode characters in position 0-1: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19612/4000966699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pandas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     cursor.execute(\"\"\"INSERT INTO Market_data(\"Open\", Day_Range, \"_52_Week_Range\", Market_Cap, Shares_Outstanding,\n\u001b[0m\u001b[0;32m     30\u001b[0m                    \u001b[0mPublic_Float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRev_per_Employee\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mP_E_Ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDividend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEx_Dividend_Date\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                    \u001b[0mShort_Interest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mof_Float_Shorted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAverage_Volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInstrument\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m?\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode characters in position 0-1: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "# Connection between the python script and the database is now been made\n",
    "# The data is overwritten to the database\n",
    "# Overwriting is done because the interest is about the data on a particular day for each instrument.\n",
    "#                                                                                      (ex: EPS, Market_cap)\n",
    "import pyodbc as db\n",
    "conn = pyodbc.connect(\n",
    "    \"Driver={ODBC Driver 17 for SQL Server};Server=localhost;Database=CSEDB;Uid=sa;Pwd=123\")\n",
    "\n",
    "conn.setdecoding(db.SQL_CHAR, encoding='latin-1')\n",
    "conn.setencoding('latin-1')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "#cursor.execute('''\n",
    "#               DELETE FROM CSEDB.dbo.Market_data_test\n",
    "#               ''')\n",
    "\n",
    "\n",
    "#for row in df.itertuples(index=True, name='Pandas'):\n",
    "#   cursor.execute('INSERT INTO Market_data(\"Open\", Day_Range, \"_52_Week_Range\", Market_Cap, Shares_Outstanding, '\n",
    "#                   'Public_Float, Beta, Rev_per_Employee, P_E_Ratio, EPS, Yield, Dividend, Ex_Dividend_Date, '\n",
    "#                   'Short_Interest, of_Float_Shorted, Average_Volume, Instrument) values(?,?,?,?,?,?,?,?,?,?,?,?,?,?,'\n",
    "#                   '?,?,?)',\n",
    "#                   row.Open, row.Day_Range, row.my_52_Week_Range, row.Market_Cap, row.Shares_Outstanding,\n",
    "#                   row.Public_Float, row.Beta, row.Rev_per_Employee, row.P_E_Ratio, row.EPS, row.Yield, row.Dividend,\n",
    "#                   row.Ex_Dividend_Date, row.Short_Interest, row.of_Float_Shorted, row.Average_Volume, row.Instrument)\n",
    "    \n",
    "for row in df.itertuples(index=True, name='Pandas'):\n",
    "    cursor.execute(\"\"\"INSERT INTO Market_data(\"Open\", Day_Range, \"_52_Week_Range\", Market_Cap, Shares_Outstanding,\n",
    "                   Public_Float, Beta, Rev_per_Employee, P_E_Ratio, EPS, Yield, Dividend, Ex_Dividend_Date,\n",
    "                   Short_Interest, of_Float_Shorted, Average_Volume, Instrument) values(?,?,?,?,?,?,?,?,?,?,?,?,?,?,\n",
    "                   ?,?,?)\"\"\",\n",
    "                   row.Open, row.Day_Range, row.my_52_Week_Range, row.Market_Cap, row.Shares_Outstanding,\n",
    "                   row.Public_Float, row.Beta, row.Rev_per_Employee, row.P_E_Ratio, row.EPS, row.Yield, row.Dividend,\n",
    "                   row.Ex_Dividend_Date, row.Short_Interest, row.of_Float_Shorted, row.Average_Volume, row.Instrument)\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c59ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a652ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.itertuples(index=True, name='Pandas'):\n",
    "    cursor.execute('INSERT INTO Market_data(\"Open\", Day_Range, \"_52_Week_Range\", Market_Cap, Shares_Outstanding, '\n",
    "                   'Public_Float, Beta, Rev_per_Employee, P_E_Ratio, EPS, Yield, Dividend, Ex_Dividend_Date, '\n",
    "                   'Short_Interest, of_Float_Shorted, Average_Volume, Instrument) values(?,?,?,?,?,?,?,?,?,?,?,?,?,?,'\n",
    "                   '?,?,?)',\n",
    "                   row.Open, row.Day_Range, row.my_52_Week_Range, row.Market_Cap, row.Shares_Outstanding,\n",
    "                   row.Public_Float, row.Beta, row.Rev_per_Employee, row.P_E_Ratio, row.EPS, row.Yield, row.Dividend,\n",
    "                   row.Ex_Dividend_Date, row.Short_Interest, row.of_Float_Shorted, row.Average_Volume, row.Instrument)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
